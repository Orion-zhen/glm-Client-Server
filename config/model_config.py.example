# 存放你所有模型的文件夹
# 例如chatglm3-6b存放在/path/to/models/chatglm3-6b
# 那么LLM_ROOT_DIR应该设置为/path/to/models
LLM_ROOT_DIR = "/path/to/models"

# 选择你要加载的模型名
LLM = "Qwen-14B-Chat" # chatglm3-6b | Qwen-14B-Chat

LLM_CARD = {
    # 模型名称, 用于加载模型
    "chatglm3-6b":
        {
            # 模型文件相对路径(相对LLM_ROOT_DIR)
            "path": "chatglm3-6b-32k",
            # 模型加载参数
            "params": [],
        },
    "Qwen-7B-Chat":
        {
            "path": "Qwen-7B-Chat-Int4",
            "params": ["--max-gpu-memory", "'20GiB'"],
            "note": "14B-Int4有时候都会爆显存, 离谱"
        },
    "WizardLM-7B-uncensored":
        {
            "path": "WizardLM-7B-uncensored-GPTQ",
            "params": ["--enable-exllama"],
        },
    "Qwen-14B-Chat":
        {
            "path": "Qwen-14B-Chat-Int4",
            "params": ["--max-gpu-memory", "'20GiB'"],
            "note": "不能开exllama, cpu版可以试用xfastertransformer; Int8开满token会爆显存"
        },
    "tigerbot-13b-chat":
        {
            "path": "tigerbot-13b-chat-v5-4bit-exl2",
            "params": [],
        },
    "OrionStar-Yi-34B":
        {
            "path": "OrionStar-Yi-34B-Chat-Llama-GPTQ",
            "params": ["--enable-exllama"],
            "note": "效果不行, 开了exllama就说个不停, 不开就跑不了; 能力也没qwen强"
        },
    "SUS-Chat-34B":
        {
            "path": "SUS-Chat-34B-GPTQ",
            "params": ["--max-gpu-memory", "'20GiB'"],
            "note": ["--enable-exllama", "--exllama-cache-8bit", "--exllama-max-seq-len", "2048"]
        }
}